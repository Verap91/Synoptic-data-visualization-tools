{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76fb7eeb",
   "metadata": {},
   "source": [
    "- EMPIRICAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40764bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3d yearly \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib import cm\n",
    "\n",
    "# Define the station order dictionary\n",
    "station_order = {\n",
    "    779: \"TP\",\n",
    "    775: \"ERICE\",\n",
    "    780: \"TRAPANI FULGATORE\",\n",
    "    778: \"SALEMI\",\n",
    "    774: \"CASTELVETRANO\",\n",
    "    773: \"CASTELLAMMARE DEL GOLFO\",\n",
    "    744: \"Contessa Entellina\",\n",
    "    751: \"PARTINICO\",\n",
    "    742: \"CAMPOREALE\",\n",
    "    749: \"MONREALE VIGNA API\",\n",
    "    745: \"CORLEONE\",\n",
    "    693: \"RIBERA\",\n",
    "    750: \"PA\",\n",
    "    686: \"BIVONA\",\n",
    "    748: \"MISILMERI\",\n",
    "    747: \"MEZZOJUSO\",\n",
    "    683: \"AG\",\n",
    "    755: \"TERMINI IMERESE\",\n",
    "    685: \"ARAGONA\",\n",
    "    684: \"AGRIGENTO MANDRASCAVA\",\n",
    "    687: \"CAMMARATA\",\n",
    "    740: \"ALIA\",\n",
    "    689: \"CANICATTì\",\n",
    "    700: \"MUSSOMELI\",\n",
    "    703: \"SCLAFANI BAGNI\",\n",
    "    690: \"LICATA\",\n",
    "    746: \"LASCARI\",\n",
    "    696: \"DELIA\",\n",
    "    754: \"POLIZZI GENEROSA\",\n",
    "    753: \"PETRALIA SOTTANA\",\n",
    "    695: \"CL\",\n",
    "    701: \"RIESI\",\n",
    "    743: \"CASTELBUONO\",\n",
    "    718: \"EN\",\n",
    "    752: \"GANGI\",\n",
    "    699: \"MAZZARINO\",\n",
    "    736: \"PETTINEO\",\n",
    "    731: \"MISTRETTA\",\n",
    "    722: \"PIAZZA ARMERINA\",\n",
    "    762: \"ACATE\",\n",
    "    721: \"NICOSIA\",\n",
    "    717: \"AIDONE\",\n",
    "    723: \"CARONIA BUZZA\",\n",
    "    760: \"SANTA CROCE CAMERINA\",\n",
    "    710: \"MAZZARRONE\",\n",
    "    716: \"CALTAGIRONE\",\n",
    "    757: \"COMISO\",\n",
    "    737: \"SAN FRATELLO\",\n",
    "    730: \"MILITELLO ROSMARINO\",\n",
    "    761: \"SCICLI\",\n",
    "    756: \"RG\",\n",
    "    725: \"CESARò VIGNAZZA\",\n",
    "    711: \"MINEO\",\n",
    "    733: \"NASO\",\n",
    "    705: \"BRONTE\",\n",
    "    712: \"PATERNò\",\n",
    "    770: \"PALAZZOLO ACREIDE\",\n",
    "    709: \"MALETTO\",\n",
    "    765: \"FRANCOFONTE\",\n",
    "    766: \"LENTINI\",\n",
    "    715: \"RANDAZZO\",\n",
    "    758: \"ISPICA\",\n",
    "    735: \"PATTI\",\n",
    "    713: \"PEDARA\",\n",
    "    767: \"NOTO\",\n",
    "    706: \"CT\",\n",
    "    769: \"PACHINO\",\n",
    "    708: \"LINGUAGLOSSA\",\n",
    "    734: \"NOVARA DI SICILIA\",\n",
    "    764: \"SR\",\n",
    "    707: \"RIPOSTO\",\n",
    "    739: \"TORREGROTTA\",\n",
    "    738: \"SAN PIER NICETO\",\n",
    "    727: \"FIUMEDINISI\",\n",
    "    729: \"ME\"\n",
    "}\n",
    "\n",
    "# Specify the stations you want to label\n",
    "stations_to_label = [\"CT\", \"PA\", \"ME\", \"AG\", \"EN\", \"CL\", \"TP\", \"SR\", \"RG\"]\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('75gauges.csv')\n",
    "\n",
    "# Convert the 'DATETIME' column to a datetime type\n",
    "df['DATETIME'] = pd.to_datetime(df['DATETIME'])\n",
    "\n",
    "df['Year'] = df['DATETIME'].dt.year\n",
    "\n",
    "# Define a colormap\n",
    "cmap = cm.colors.ListedColormap(['#9ABDDC', '#B4CF68', '#FFD872', '#FF96C5', '#FF00FF', 'purple'])\n",
    "\n",
    "# Create a list of station IDs in the desired order\n",
    "station_ids_in_order = list(station_order.keys())\n",
    "\n",
    "fig = plt.figure(figsize=(7, 7))\n",
    "ax = fig.add_subplot(111, projection='3d')  # Create a single 3D subplot\n",
    "\n",
    "filtered_df = df[(df['VALUE'] > 1) & (df['VALUE'] < 2000)]\n",
    "pivot_table = filtered_df.pivot_table(index='STATION', columns='Year', values='VALUE', aggfunc='max').fillna(0)\n",
    "\n",
    "# Reorder the pivot_table and Y_labels based on station_order\n",
    "pivot_table = pivot_table.loc[station_ids_in_order]\n",
    "\n",
    "# Create a list of y-axis labels\n",
    "Y_labels = []\n",
    "for station_id in station_ids_in_order:\n",
    "    station_name = station_order[station_id]\n",
    "    if station_name.upper() in stations_to_label:\n",
    "        Y_labels.append(station_name)  # Label specific cities\n",
    "    else:\n",
    "        Y_labels.append(\"\")  # Empty string for other stations\n",
    "\n",
    "# Calculate the normalization for the data\n",
    "norm = Normalize(vmin=pivot_table.values.min(), vmax=pivot_table.values.max())\n",
    "\n",
    "# Apply the normalization to the count values and map to the 'coolwarm' colormap\n",
    "colors = cmap(norm(pivot_table.values.ravel()))\n",
    "\n",
    "Z = pivot_table.values\n",
    "X_labels = pivot_table.columns[::1]\n",
    "\n",
    "X, Y = np.meshgrid(np.arange(len(X_labels)), np.arange(len(Y_labels)))\n",
    "\n",
    "dx = np.ones(Z.shape) * 0.75\n",
    "dy = np.ones(Z.shape) * 0.75\n",
    "dz = Z\n",
    "\n",
    "for x, y, z, color in zip(X.flatten(), Y.flatten(), dz.flatten(), colors):\n",
    "    ax.bar3d(x, y, 0, dx[0, 0], dy[0, 0], z, shade=True, color=color)\n",
    "\n",
    "ax.set_title('Yearly Sum Volumes')\n",
    "ax.set_zlabel('mm')\n",
    "#ax.set_zlim(0, 500)\n",
    "ax.set_yticks(np.arange(len(Y_labels)) + 5.5)\n",
    "ax.set_yticklabels(Y_labels, fontsize= 7)\n",
    "ax.set_xticks(np.arange(len(X_labels)))\n",
    "ax.set_xticklabels(X_labels, rotation=45)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.05, 1, 0.95])\n",
    "#fig.savefig(\"75gauge_3d_yearly_volumes.jpg\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60f53d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2d yearly \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib import cm\n",
    "\n",
    "# Define the station order dictionary\n",
    "station_order = {\n",
    "    729: \"MESSINA\",\n",
    "    727: \"FIUMEDINISI\",\n",
    "    738: \"SAN PIER NICETO\",\n",
    "    739: \"TORREGROTTA\",\n",
    "    707: \"RIPOSTO\",\n",
    "    764: \"SIRACUSA\",\n",
    "    734: \"NOVARA DI SICILIA\",\n",
    "    708: \"LINGUAGLOSSA\",\n",
    "    769: \"PACHINO\",\n",
    "    706: \"CATANIA\",\n",
    "    767: \"NOTO\",\n",
    "    713: \"PEDARA\",\n",
    "    735: \"PATTI\",\n",
    "    758: \"ISPICA\",\n",
    "    715: \"RANDAZZO\",\n",
    "    766: \"LENTINI\",\n",
    "    765: \"FRANCOFONTE\",\n",
    "    709: \"MALETTO\",\n",
    "    770: \"PALAZZOLO ACREIDE\",\n",
    "    712: \"PATERNò\",\n",
    "    705: \"BRONTE\",\n",
    "    733: \"NASO\",\n",
    "    711: \"MINEO\",\n",
    "    725: \"CESARò VIGNAZZA\",\n",
    "    756: \"RAGUSA\",\n",
    "    761: \"SCICLI\",\n",
    "    730: \"MILITELLO ROSMARINO\",\n",
    "    737: \"SAN FRATELLO\",\n",
    "    757: \"COMISO\",\n",
    "    716: \"CALTAGIRONE\",\n",
    "    710: \"MAZZARRONE\",\n",
    "    760: \"SANTA CROCE CAMERINA\",\n",
    "    723: \"CARONIA BUZZA\",\n",
    "    717: \"AIDONE\",\n",
    "    721: \"NICOSIA\",\n",
    "    762: \"ACATE\",\n",
    "    722: \"PIAZZA ARMERINA\",\n",
    "    731: \"MISTRETTA\",\n",
    "    736: \"PETTINEO\",\n",
    "    699: \"MAZZARINO\",\n",
    "    752: \"GANGI\",\n",
    "    718: \"ENNA\",\n",
    "    743: \"CASTELBUONO\",\n",
    "    701: \"RIESI\",\n",
    "    695: \"CALTANISSETTA\",\n",
    "    753: \"PETRALIA SOTTANA\",\n",
    "    754: \"POLIZZI GENEROSA\",\n",
    "    696: \"DELIA\",\n",
    "    746: \"LASCARI\",\n",
    "    690: \"LICATA\",\n",
    "    703: \"SCLAFANI BAGNI\",\n",
    "    700: \"MUSSOMELI\",\n",
    "    689: \"CANICATTì\",\n",
    "    740: \"ALIA\",\n",
    "    687: \"CAMMARATA\",\n",
    "    684: \"AGRIGENTO MANDRASCAVA\",\n",
    "    685: \"ARAGONA\",\n",
    "    755: \"TERMINI IMERESE\",\n",
    "    683: \"AGRIGENTO SCIBICA\",\n",
    "    747: \"MEZZOJUSO\",\n",
    "    748: \"MISILMERI\",\n",
    "    686: \"BIVONA\",\n",
    "    750: \"PALERMO\",\n",
    "    693: \"RIBERA\",\n",
    "    745: \"CORLEONE\",\n",
    "    749: \"MONREALE VIGNA API\",\n",
    "    742: \"CAMPOREALE\",\n",
    "    751: \"PARTINICO\",\n",
    "    744: \"Contessa Entellina\",\n",
    "    773: \"CASTELLAMMARE DEL GOLFO\",\n",
    "    774: \"CASTELVETRANO\",\n",
    "    778: \"SALEMI\",\n",
    "    780: \"TRAPANI FULGATORE\",\n",
    "    775: \"ERICE\",\n",
    "    779: \"TRAPANI FONTANASALSA\"\n",
    "}\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('75gauges.csv')\n",
    "\n",
    "# Convert the 'DATETIME' column to a datetime type\n",
    "df['DATETIME'] = pd.to_datetime(df['DATETIME'])\n",
    "\n",
    "df['Year'] = df['DATETIME'].dt.year\n",
    "\n",
    "# Define a custom colormap with specific colors from 'tab10'\n",
    "cmap = cm.colors.ListedColormap(['#9ABDDC','#B4CF68','#FFD872', '#FF96C5','#FF00FF','purple'])\n",
    "\n",
    "# Create a list of station IDs in the desired order\n",
    "station_ids_in_order = list(station_order.keys())\n",
    "\n",
    "# Create a single plot for yearly data per station\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "filtered_df = df[(df['VALUE'] > 1) & (df['VALUE'] < 2000)]\n",
    "\n",
    "pivot_table = filtered_df.pivot_table(index='STATION', columns='Year', values='VALUE', aggfunc='max').fillna(0)\n",
    "\n",
    "# Reorder the pivot_table based on station_order\n",
    "pivot_table = pivot_table.loc[station_ids_in_order]\n",
    "\n",
    "# Create a list of y-axis labels\n",
    "Y_labels = [station_order[station_id] for station_id in station_ids_in_order]\n",
    "\n",
    "# Calculate the normalization for the data\n",
    "norm = Normalize(vmin=pivot_table.values.min(), vmax=pivot_table.values.max())\n",
    "\n",
    "# Apply the normalization to the count values and map to the custom colormap\n",
    "colors = cmap(norm(pivot_table.values))\n",
    "\n",
    "# Create X and Y data for the heatmap\n",
    "X_labels = pivot_table.columns\n",
    "X, Y = np.meshgrid(np.arange(len(X_labels)), np.arange(len(Y_labels)))\n",
    "\n",
    "# Plot the heatmap\n",
    "heatmap = ax.imshow(pivot_table.values, cmap=cmap, norm=norm)\n",
    "ax.set_title('Yearly Data per Station')\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Station')\n",
    "ax.set_xticks(np.arange(len(X_labels)))\n",
    "ax.set_xticklabels(X_labels, rotation=90, fontsize= 5)\n",
    "ax.set_yticks(np.arange(len(Y_labels)))\n",
    "ax.set_yticklabels(Y_labels, rotation=0 , fontsize= 5)\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(heatmap, ax=ax)\n",
    "cbar.set_label('mm')\n",
    "\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.05, 1, 0.95])\n",
    "#fig.savefig(\"75gauge_3d_yearlyvolumessheatmap.jpg\", dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3bb0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3d seasonal\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib import cm\n",
    "\n",
    "# Define the station order dictionary\n",
    "station_order = {\n",
    "    779: \"TP\",\n",
    "    775: \"ERICE\",\n",
    "    780: \"TRAPANI FULGATORE\",\n",
    "    778: \"SALEMI\",\n",
    "    774: \"CASTELVETRANO\",\n",
    "    773: \"CASTELLAMMARE DEL GOLFO\",\n",
    "    744: \"Contessa Entellina\",\n",
    "    751: \"PARTINICO\",\n",
    "    742: \"CAMPOREALE\",\n",
    "    749: \"MONREALE VIGNA API\",\n",
    "    745: \"CORLEONE\",\n",
    "    693: \"RIBERA\",\n",
    "    750: \"PA\",\n",
    "    686: \"BIVONA\",\n",
    "    748: \"MISILMERI\",\n",
    "    747: \"MEZZOJUSO\",\n",
    "    683: \"AG\",\n",
    "    755: \"TERMINI IMERESE\",\n",
    "    685: \"ARAGONA\",\n",
    "    684: \"AGRIGENTO MANDRASCAVA\",\n",
    "    687: \"CAMMARATA\",\n",
    "    740: \"ALIA\",\n",
    "    689: \"CANICATTì\",\n",
    "    700: \"MUSSOMELI\",\n",
    "    703: \"SCLAFANI BAGNI\",\n",
    "    690: \"LICATA\",\n",
    "    746: \"LASCARI\",\n",
    "    696: \"DELIA\",\n",
    "    754: \"POLIZZI GENEROSA\",\n",
    "    753: \"PETRALIA SOTTANA\",\n",
    "    695: \"CL\",\n",
    "    701: \"RIESI\",\n",
    "    743: \"CASTELBUONO\",\n",
    "    718: \"EN\",\n",
    "    752: \"GANGI\",\n",
    "    699: \"MAZZARINO\",\n",
    "    736: \"PETTINEO\",\n",
    "    731: \"MISTRETTA\",\n",
    "    722: \"PIAZZA ARMERINA\",\n",
    "    762: \"ACATE\",\n",
    "    721: \"NICOSIA\",\n",
    "    717: \"AIDONE\",\n",
    "    723: \"CARONIA BUZZA\",\n",
    "    760: \"SANTA CROCE CAMERINA\",\n",
    "    710: \"MAZZARRONE\",\n",
    "    716: \"CALTAGIRONE\",\n",
    "    757: \"COMISO\",\n",
    "    737: \"SAN FRATELLO\",\n",
    "    730: \"MILITELLO ROSMARINO\",\n",
    "    761: \"SCICLI\",\n",
    "    756: \"RG\",\n",
    "    725: \"CESARò VIGNAZZA\",\n",
    "    711: \"MINEO\",\n",
    "    733: \"NASO\",\n",
    "    705: \"BRONTE\",\n",
    "    712: \"PATERNò\",\n",
    "    770: \"PALAZZOLO ACREIDE\",\n",
    "    709: \"MALETTO\",\n",
    "    765: \"FRANCOFONTE\",\n",
    "    766: \"LENTINI\",\n",
    "    715: \"RANDAZZO\",\n",
    "    758: \"ISPICA\",\n",
    "    735: \"PATTI\",\n",
    "    713: \"PEDARA\",\n",
    "    767: \"NOTO\",\n",
    "    706: \"CT\",\n",
    "    769: \"PACHINO\",\n",
    "    708: \"LINGUAGLOSSA\",\n",
    "    734: \"NOVARA DI SICILIA\",\n",
    "    764: \"SR\",\n",
    "    707: \"RIPOSTO\",\n",
    "    739: \"TORREGROTTA\",\n",
    "    738: \"SAN PIER NICETO\",\n",
    "    727: \"FIUMEDINISI\",\n",
    "    729: \"ME\"\n",
    "}\n",
    "\n",
    "# Specify the stations you want to label\n",
    "stations_to_label = [\"CT\", \"PA\", \"ME\", \"AG\", \"EN\", \"CL\", \"TP\", \"SR\", \"RG\"]\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('75gauges.csv')\n",
    "\n",
    "# Convert the 'DATETIME' column to a datetime type\n",
    "df['DATETIME'] = pd.to_datetime(df['DATETIME'])\n",
    "\n",
    "def map_month_to_season(month):\n",
    "    if month in [3, 4, 5]:\n",
    "        return 'Spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Summer'\n",
    "    elif month in [9, 10, 11]:\n",
    "        return 'Autumn'\n",
    "    else:\n",
    "        return 'Winter'\n",
    "\n",
    "df['Year'] = df['DATETIME'].dt.year\n",
    "df['Month'] = df['DATETIME'].dt.month\n",
    "df['Season'] = df['Month'].apply(map_month_to_season)\n",
    "\n",
    "# Define a colormap\n",
    "#cmap = cm.get_cmap('Paired',6)\n",
    "cmap = cm.colors.ListedColormap(['#9ABDDC','#B4CF68','#FFD872', '#FF96C5','#FF00FF','purple'])\n",
    "\n",
    "# Create a list of station IDs in the desired order\n",
    "station_ids_in_order = list(station_order.keys())\n",
    "\n",
    "unique_seasons = df['Season'].unique()\n",
    "\n",
    "fig = plt.figure(figsize=(22, 22))\n",
    "\n",
    "for i, season in enumerate(unique_seasons, 1):\n",
    "    ax = fig.add_subplot(2, 2, i, projection='3d')\n",
    "    \n",
    "    filtered_df = df[(df['Season'] == season) & (df['VALUE'] > 1) & (df['VALUE'] < 2000)]\n",
    "    pivot_table = filtered_df.pivot_table(index='STATION', columns='Year', values='VALUE', aggfunc='sum').fillna(0)\n",
    "    \n",
    "    # Reorder the pivot_table and Y_labels based on station_order\n",
    "    pivot_table = pivot_table.loc[station_ids_in_order]\n",
    "    \n",
    "    # Create a list of y-axis labels\n",
    "    Y_labels = []\n",
    "    for station_id in station_ids_in_order:\n",
    "        station_name = station_order[station_id]\n",
    "        if station_name.upper() in stations_to_label:\n",
    "            Y_labels.append(station_name)  # Label specific cities\n",
    "        else:\n",
    "            Y_labels.append(\"\")  # Empty string for other stations\n",
    "    \n",
    "    # Calculate the normalization for this season's data\n",
    "    norm = Normalize(vmin=pivot_table.values.min(), vmax=pivot_table.values.max())\n",
    "    \n",
    "    # Apply the normalization to the count values and map to the 'coolwarm' colormap\n",
    "    colors = cmap(norm(pivot_table.values.ravel()))\n",
    "\n",
    "    Z = pivot_table.values\n",
    "    X_labels = pivot_table.columns[::1]\n",
    "    \n",
    "    X, Y = np.meshgrid(np.arange(len(X_labels)), np.arange(len(Y_labels)))\n",
    "    \n",
    "    dx = np.ones(Z.shape) * 0.75\n",
    "    dy = np.ones(Z.shape) * 0.75\n",
    "    dz = Z\n",
    "    \n",
    "    for x, y, z, color in zip(X.flatten(), Y.flatten(), dz.flatten(), colors):\n",
    "        #ax.bar3d(x, y, 0, dx[0, 0], dy[0, 0], z, shade=True, color=color, edgecolor='k')\n",
    "        ax.bar3d(x, y, 0, dx[0, 0], dy[0, 0], z, shade=True, color=color)\n",
    "\n",
    "    ax.set_title(f'Volumes {season}')\n",
    "    ax.set_zlabel('mm')\n",
    "    ax.set_zlim(0, 1000)\n",
    "    ax.set_yticks(np.arange(len(Y_labels))+7.5)\n",
    "    ax.set_yticklabels(Y_labels)\n",
    "    ax.set_xticks(np.arange(len(X_labels)))\n",
    "    ax.set_xticklabels(X_labels, rotation=45)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.05, 1, 0.95]) \n",
    "#fig.savefig(\"75gauge_3d_volumes.jpg\", dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac27087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a colormap\n",
    "cmap = cm.colors.ListedColormap(['#9ABDDC', '#B4CF68', '#FFD872', '#FF96C5', '#FF00FF', 'purple'])\n",
    "\n",
    "# Create a list of station IDs in the desired order\n",
    "station_ids_in_order = list(station_order.keys())\n",
    "\n",
    "unique_seasons = df['Season'].unique()\n",
    "\n",
    "# Loop through each season and save the plot separately\n",
    "for i, season in enumerate(unique_seasons):\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    filtered_df = df[(df['Season'] == season) & (df['VALUE'] > 1) & (df['VALUE'] < 2000)]\n",
    "    pivot_table = filtered_df.pivot_table(index='STATION', columns='Year', values='VALUE', aggfunc='sum').fillna(0)\n",
    "    \n",
    "    # Reorder the pivot_table and Y_labels based on station_order\n",
    "    pivot_table = pivot_table.loc[station_ids_in_order]\n",
    "    \n",
    "    # Create a list of y-axis labels\n",
    "    Y_labels = []\n",
    "    for station_id in station_ids_in_order:\n",
    "        station_name = station_order[station_id]\n",
    "        if station_name.upper() in stations_to_label:\n",
    "            Y_labels.append(station_name)  # Label specific cities\n",
    "        else:\n",
    "            Y_labels.append(\"\")  # Empty string for other stations\n",
    "    \n",
    "    # Calculate the normalization for this season's data\n",
    "    norm = Normalize(vmin=pivot_table.values.min(), vmax=pivot_table.values.max())\n",
    "    \n",
    "    # Apply the normalization to the count values and map to the colormap\n",
    "    colors = cmap(norm(pivot_table.values.ravel()))\n",
    "\n",
    "    Z = pivot_table.values\n",
    "    X_labels = pivot_table.columns\n",
    "    X, Y = np.meshgrid(np.arange(len(X_labels)), np.arange(len(Y_labels)))\n",
    "    \n",
    "    dx = np.ones(Z.shape) * 0.75\n",
    "    dy = np.ones(Z.shape) * 0.75\n",
    "    dz = Z\n",
    "    \n",
    "    for x, y, z, color in zip(X.flatten(), Y.flatten(), dz.flatten(), colors):\n",
    "        ax.bar3d(x, y, 0, dx[0, 0], dy[0, 0], z, shade=True, color=color)\n",
    "    \n",
    "    ax.set_title(f'Volumes {season}', fontsize=20)\n",
    "    ax.set_zlabel('mm')\n",
    "    ax.set_zlim(0, 1000)\n",
    "    ax.set_yticks(np.arange(len(Y_labels)))\n",
    "    ax.set_yticklabels(Y_labels)\n",
    "    ax.set_xticks(np.arange(len(X_labels)))\n",
    "    ax.set_xticklabels(X_labels, rotation=45)\n",
    "    \n",
    "    # Save each plot as a separate file\n",
    "    fig.savefig(f\"75gauge_3d_volumes_{season}.jpg\", dpi=500)\n",
    "    plt.close(fig)  # Close the figure to avoid overlapping plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d27f021",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2d seasonal\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib import cm\n",
    "\n",
    "# Define the station order dictionary\n",
    "station_order = {\n",
    "    729: \"MESSINA\",\n",
    "    727: \"FIUMEDINISI\",\n",
    "    738: \"SAN PIER NICETO\",\n",
    "    739: \"TORREGROTTA\",\n",
    "    707: \"RIPOSTO\",\n",
    "    764: \"SIRACUSA\",\n",
    "    734: \"NOVARA DI SICILIA\",\n",
    "    708: \"LINGUAGLOSSA\",\n",
    "    769: \"PACHINO\",\n",
    "    706: \"CATANIA\",\n",
    "    767: \"NOTO\",\n",
    "    713: \"PEDARA\",\n",
    "    735: \"PATTI\",\n",
    "    758: \"ISPICA\",\n",
    "    715: \"RANDAZZO\",\n",
    "    766: \"LENTINI\",\n",
    "    765: \"FRANCOFONTE\",\n",
    "    709: \"MALETTO\",\n",
    "    770: \"PALAZZOLO ACREIDE\",\n",
    "    712: \"PATERNò\",\n",
    "    705: \"BRONTE\",\n",
    "    733: \"NASO\",\n",
    "    711: \"MINEO\",\n",
    "    725: \"CESARò VIGNAZZA\",\n",
    "    756: \"RAGUSA\",\n",
    "    761: \"SCICLI\",\n",
    "    730: \"MILITELLO ROSMARINO\",\n",
    "    737: \"SAN FRATELLO\",\n",
    "    757: \"COMISO\",\n",
    "    716: \"CALTAGIRONE\",\n",
    "    710: \"MAZZARRONE\",\n",
    "    760: \"SANTA CROCE CAMERINA\",\n",
    "    723: \"CARONIA BUZZA\",\n",
    "    717: \"AIDONE\",\n",
    "    721: \"NICOSIA\",\n",
    "    762: \"ACATE\",\n",
    "    722: \"PIAZZA ARMERINA\",\n",
    "    731: \"MISTRETTA\",\n",
    "    736: \"PETTINEO\",\n",
    "    699: \"MAZZARINO\",\n",
    "    752: \"GANGI\",\n",
    "    718: \"ENNA\",\n",
    "    743: \"CASTELBUONO\",\n",
    "    701: \"RIESI\",\n",
    "    695: \"CALTANISSETTA\",\n",
    "    753: \"PETRALIA SOTTANA\",\n",
    "    754: \"POLIZZI GENEROSA\",\n",
    "    696: \"DELIA\",\n",
    "    746: \"LASCARI\",\n",
    "    690: \"LICATA\",\n",
    "    703: \"SCLAFANI BAGNI\",\n",
    "    700: \"MUSSOMELI\",\n",
    "    689: \"CANICATTì\",\n",
    "    740: \"ALIA\",\n",
    "    687: \"CAMMARATA\",\n",
    "    684: \"AGRIGENTO MANDRASCAVA\",\n",
    "    685: \"ARAGONA\",\n",
    "    755: \"TERMINI IMERESE\",\n",
    "    683: \"AGRIGENTO SCIBICA\",\n",
    "    747: \"MEZZOJUSO\",\n",
    "    748: \"MISILMERI\",\n",
    "    686: \"BIVONA\",\n",
    "    750: \"PALERMO\",\n",
    "    693: \"RIBERA\",\n",
    "    745: \"CORLEONE\",\n",
    "    749: \"MONREALE VIGNA API\",\n",
    "    742: \"CAMPOREALE\",\n",
    "    751: \"PARTINICO\",\n",
    "    744: \"Contessa Entellina\",\n",
    "    773: \"CASTELLAMMARE DEL GOLFO\",\n",
    "    774: \"CASTELVETRANO\",\n",
    "    778: \"SALEMI\",\n",
    "    780: \"TRAPANI FULGATORE\",\n",
    "    775: \"ERICE\",\n",
    "    779: \"TRAPANI FONTANASALSA\"\n",
    "}\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('75gauges.csv')\n",
    "\n",
    "# Convert the 'DATETIME' column to a datetime type\n",
    "df['DATETIME'] = pd.to_datetime(df['DATETIME'])\n",
    "\n",
    "def map_month_to_season(month):\n",
    "    if month in [3, 4, 5]:\n",
    "        return 'Spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Summer'\n",
    "    elif month in [9, 10, 11]:\n",
    "        return 'Autumn'\n",
    "    else:\n",
    "        return 'Winter'\n",
    "\n",
    "df['Year'] = df['DATETIME'].dt.year\n",
    "df['Month'] = df['DATETIME'].dt.month\n",
    "df['Season'] = df['Month'].apply(map_month_to_season)\n",
    "\n",
    "# Define a colormap\n",
    "# Define a custom colormap with specific colors from 'tab10'\n",
    "cmap = cm.colors.ListedColormap(['#9ABDDC','#B4CF68','#FFD872', '#FF96C5','#FF00FF','purple'])\n",
    "#cmap = cm.get_cmap('Paired',6)\n",
    "#'#FFAAB0'\n",
    "\n",
    "# Create a list of station IDs in the desired order\n",
    "station_ids_in_order = list(station_order.keys())\n",
    "\n",
    "unique_seasons = df['Season'].unique()\n",
    "\n",
    "# Calculate the number of rows and columns for subplots based on the number of seasons\n",
    "num_seasons = len(unique_seasons)\n",
    "num_rows = int(np.ceil(num_seasons / 2))\n",
    "num_cols = 2\n",
    "\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(22, 22))\n",
    "\n",
    "for i, season in enumerate(unique_seasons, 1):\n",
    "    row_index = (i - 1) // num_cols\n",
    "    col_index = (i - 1) % num_cols\n",
    "    ax = axes[row_index, col_index]\n",
    "    \n",
    "    filtered_df = df[(df['Season'] == season) & (df['VALUE'] > 1) & (df['VALUE'] < 2000)]\n",
    "    pivot_table = filtered_df.pivot_table(index='STATION', columns='Year', values='VALUE', aggfunc='sum').fillna(0)\n",
    "    \n",
    "    # Reorder the pivot_table based on station_order\n",
    "    pivot_table = pivot_table.loc[station_ids_in_order]\n",
    "    \n",
    "    # Create a list of y-axis labels\n",
    "    Y_labels = [station_order[station_id] for station_id in station_ids_in_order]\n",
    "    \n",
    "    # Calculate the normalization for this season's data\n",
    "    norm = Normalize(vmin=pivot_table.values.min(), vmax=pivot_table.values.max())\n",
    "    \n",
    "    # Apply the normalization to the count values and map to the 'coolwarm' colormap\n",
    "    colors = cmap(norm(pivot_table.values))\n",
    "    \n",
    "    # Create X and Y data for the heatmap\n",
    "    X_labels = pivot_table.columns[::1]\n",
    "    X, Y = np.meshgrid(np.arange(len(X_labels)), np.arange(len(Y_labels)))\n",
    "    \n",
    "    # Plot the heatmap\n",
    "    heatmap = ax.imshow(pivot_table.values, cmap=cmap, norm=norm)\n",
    "    ax.set_title(f'Volumes {season}')\n",
    "    ax.set_xlabel('Year')\n",
    "    ax.set_ylabel('Station')\n",
    "    ax.set_xticks(np.arange(len(X_labels)))\n",
    "    ax.set_xticklabels(X_labels, rotation=90)\n",
    "    ax.set_yticks(np.arange(len(Y_labels)))\n",
    "    ax.set_yticklabels(Y_labels, rotation=0)\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(heatmap, ax=ax)\n",
    "    cbar.set_label('mm')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.05, 1, 0.95])\n",
    "fig.savefig(\"75gauge_3d_volumesheatmap.jpg\", dpi=300)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176c87de",
   "metadata": {},
   "source": [
    "NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8547fb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all networks grid visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Circle\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "\n",
    "# Load data\n",
    "gauges_data = pd.read_csv('75gauges12H.csv')\n",
    "coordinates_data = pd.read_csv('merged_COO.csv')\n",
    "\n",
    "# Merge data on STATION\n",
    "merged_data = pd.merge(gauges_data, coordinates_data, on='STATION')\n",
    "\n",
    "# Convert 'DATETIME' to datetime\n",
    "merged_data['DATETIME'] = pd.to_datetime(merged_data['DATETIME'])\n",
    "\n",
    "# Define the station IDs you want to include in the network\n",
    "stations_of_interest = [779, 750, 684, 695, 718, 756, 706, 764, 729]\n",
    "\n",
    "# Filter merged_data to include only stations of interest\n",
    "merged_data_filtered = merged_data[merged_data['STATION'].isin(stations_of_interest)]\n",
    "\n",
    "# Granger causality test function\n",
    "def granger_test(dataframe, column1, column2, max_lag=1):\n",
    "    try:\n",
    "        gc_test = grangercausalitytests(dataframe[[column1, column2]], maxlag=max_lag)\n",
    "        f_statistic = gc_test[1][0]['ssr_ftest'][0]  # F-statistic\n",
    "        p_value = gc_test[1][0]['ssr_ftest'][1]  # p-value\n",
    "        return f_statistic if p_value < 0.01 else 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error in granger_test: {e}\")\n",
    "        return 0\n",
    "\n",
    "# Initialize variables to find global maxima\n",
    "global_max_out_links = 0\n",
    "global_max_in_links = 0\n",
    "global_max_f_statistic_out = 0\n",
    "global_max_f_statistic_in = 0\n",
    "station_year_data_out = {}\n",
    "station_year_data_in = {}\n",
    "\n",
    "# Loop over each year for analysis\n",
    "for year in range(merged_data_filtered['DATETIME'].dt.year.min(), merged_data_filtered['DATETIME'].dt.year.max() + 1):\n",
    "    data_year = merged_data_filtered[(merged_data_filtered['DATETIME'].dt.year == year) & \n",
    "                                     (merged_data_filtered['DATETIME'].dt.month.isin([12, 1, 2]))]\n",
    "\n",
    "    # Replace zero values with a small quantity\n",
    "    small_quantity = 0.000001\n",
    "    data_year.loc[:, 'VALUE'] = data_year['VALUE'].replace(0, small_quantity)\n",
    "\n",
    "    # Calculate log returns\n",
    "    def calculate_log_returns(group):\n",
    "        log_returns = np.log(group) - np.log(group.shift(1))\n",
    "        return log_returns\n",
    "\n",
    "    data_year.loc[:, 'LOG_RETURN'] = data_year.groupby('STATION')['VALUE'].transform(calculate_log_returns)\n",
    "    data_year = data_year.dropna()\n",
    "\n",
    "    # Reset index to align with the original DataFrame\n",
    "    data_year.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Pivot the filtered data\n",
    "    pivot_data = data_year.pivot(index='DATETIME', columns='STATION', values='LOG_RETURN').fillna(0)\n",
    "\n",
    "    temp_out_links_count = {station: 0 for station in pivot_data.columns}\n",
    "    temp_in_links_count = {station: 0 for station in pivot_data.columns}\n",
    "    temp_f_statistics_sum_out = {station: 0 for station in pivot_data.columns}\n",
    "    temp_f_statistics_sum_in = {station: 0 for station in pivot_data.columns}\n",
    "\n",
    "    # Analysis for each station pair\n",
    "    for station1 in pivot_data.columns:\n",
    "        for station2 in pivot_data.columns:\n",
    "            if station1 != station2:\n",
    "                f_statistic_out = granger_test(pivot_data, station1, station2)\n",
    "                f_statistic_in = granger_test(pivot_data, station2, station1)\n",
    "                \n",
    "                if f_statistic_out > 0:\n",
    "                    temp_out_links_count[station1] += 1\n",
    "                    temp_f_statistics_sum_out[station1] += f_statistic_out\n",
    "                    station_year_data_out[(station1, year)] = (temp_f_statistics_sum_out[station1], temp_out_links_count[station1])\n",
    "                \n",
    "                if f_statistic_in > 0:\n",
    "                    temp_in_links_count[station1] += 1\n",
    "                    temp_f_statistics_sum_in[station1] += f_statistic_in\n",
    "                    station_year_data_in[(station1, year)] = (temp_f_statistics_sum_in[station1], temp_in_links_count[station1])\n",
    "\n",
    "    # Update global maxima\n",
    "    year_max_out_links = max(temp_out_links_count.values())\n",
    "    year_max_in_links = max(temp_in_links_count.values())\n",
    "    year_max_f_statistic_out = max(temp_f_statistics_sum_out.values())\n",
    "    year_max_f_statistic_in = max(temp_f_statistics_sum_in.values())\n",
    "    \n",
    "    global_max_out_links = max(global_max_out_links, year_max_out_links)\n",
    "    global_max_in_links = max(global_max_in_links, year_max_in_links)\n",
    "    global_max_f_statistic_out = max(global_max_f_statistic_out, year_max_f_statistic_out)\n",
    "    global_max_f_statistic_in = max(global_max_f_statistic_in, year_max_f_statistic_in)\n",
    "\n",
    "# Define the station order dictionary\n",
    "station_order = {\n",
    "    779: \"TP\",\n",
    "    750: \"PA\",\n",
    "    684: \"AG\",\n",
    "    695: \"CL\",\n",
    "    718: \"EN\",\n",
    "    756: \"RG\",\n",
    "    706: \"CT\",\n",
    "    764: \"SR\",\n",
    "    729: \"ME\"\n",
    "}  # ESTOVEST\n",
    "\n",
    "def create_visualization(station_year_data, global_max_links, global_max_f_statistic, title, filename, fontsize=12):\n",
    "    fig, ax = plt.subplots(figsize=(15, 10))\n",
    "    ax.set_facecolor('white')  # Set background to white\n",
    "    \n",
    "    # Define scales for size and color\n",
    "    max_node_size = 0.2  # Maximum node size\n",
    "    color_norm = plt.Normalize(0, global_max_links)  # Normalize link count\n",
    "    color_map = plt.cm.coolwarm  # Color map\n",
    "\n",
    "    # Draw each station-year as a circle on the grid\n",
    "    for (station, year), (f_stat, links) in station_year_data.items():\n",
    "        x = year\n",
    "        y = list(station_order.keys()).index(station)  # Get station position based on order\n",
    "        size = (f_stat / global_max_f_statistic) * max_node_size  # Scale size based on f_stat\n",
    "        color = color_map(color_norm(links))  # Get color based on number of links\n",
    "\n",
    "        # Create a circle and add it to the plot\n",
    "        circle = Circle((x, y), np.sqrt(size), color=color, alpha=0.6)  # Use square root of size for radius\n",
    "        ax.add_patch(circle)\n",
    "\n",
    "    # Add color bar\n",
    "    sm = plt.cm.ScalarMappable(cmap=color_map, norm=color_norm)\n",
    "    sm.set_array([])\n",
    "    cbar = plt.colorbar(sm, ax=ax, orientation='vertical', pad=0.02)\n",
    "    cbar.set_label('Number of Links', fontsize=12)\n",
    "    cbar.set_ticks(range(0, global_max_links + 1))  # Set integer ticks on the color bar\n",
    "\n",
    "    # Set axis labels, ticks, and limits\n",
    "    ax.set_xlabel('Year')\n",
    "    ax.set_ylabel('Station ID')\n",
    "    ax.set_xticks(np.arange(merged_data_filtered['DATETIME'].dt.year.min(), merged_data_filtered['DATETIME'].dt.year.max() + 1))\n",
    "    ax.set_yticks(np.arange(len(station_order)))\n",
    "    ax.set_yticklabels(station_order.values())\n",
    "    ax.set_xlim(merged_data_filtered['DATETIME'].dt.year.min() - 1, merged_data_filtered['DATETIME'].dt.year.max() + 1)\n",
    "    ax.set_ylim(-1, len(station_order))\n",
    "\n",
    "    plt.grid(False)\n",
    "    plt.title(title, fontsize=fontsize)\n",
    "    plt.savefig(filename, dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# Create visualizations for outlinks and inlinks\n",
    "# Create visualizations for outlinks and inlinks\n",
    "create_visualization(station_year_data_out, global_max_out_links, global_max_f_statistic_out, 'Winter Outlinks - 12h',\n",
    "                     \"1.12Hgrid_visualization9gauges.jpg\", fontsize=14)\n",
    "create_visualization(station_year_data_in, global_max_in_links, global_max_f_statistic_in, 'Winter Inlinks - 12h',\n",
    "                     \"1.12Hingrid_visualization9gauges.jpg\", fontsize=14)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11f4edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# netwrok string visualization\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "\n",
    "# Load data\n",
    "#gauges_data = pd.read_csv('combined_data_9gauges.csv')\n",
    "#gauges_data = pd.read_csv('75gauges.csv')\n",
    "#gauges_data = pd.read_csv('75gauges6H.csv')\n",
    "#gauges_data = pd.read_csv('75gauges12H.csv')\n",
    "gauges_data = pd.read_csv('75gauges24H.csv')\n",
    "coordinates_data = pd.read_csv('merged_COO.csv')\n",
    "\n",
    "# Merge data on STATION\n",
    "merged_data = pd.merge(gauges_data, coordinates_data, on='STATION')\n",
    "\n",
    "# Convert 'DATETIME' to datetime\n",
    "merged_data['DATETIME'] = pd.to_datetime(merged_data['DATETIME'])\n",
    "\n",
    "# Define the station IDs you want to include in the network\n",
    "stations_of_interest = [779, 750, 684, 695, 718, 756, 706, 764, 729]\n",
    "\n",
    "# Filter merged_data to include only stations of interest\n",
    "merged_data_filtered = merged_data[merged_data['STATION'].isin(stations_of_interest)]\n",
    "\n",
    "# Granger causality test function\n",
    "def granger_test(dataframe, column1, column2, max_lag=1):\n",
    "    try:\n",
    "        gc_test = grangercausalitytests(dataframe[[column1, column2]], maxlag=max_lag)\n",
    "        f_statistic = gc_test[1][0]['ssr_ftest'][0]  # F-statistic\n",
    "        p_value = gc_test[1][0]['ssr_ftest'][1]  # p-value\n",
    "        return f_statistic if p_value < 0.01 else 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error in granger_test: {e}\")\n",
    "        return 0\n",
    "\n",
    "# Function to calculate log returns\n",
    "def calculate_log_returns(group):\n",
    "    log_returns = np.log(group) - np.log(group.shift(1))\n",
    "    return log_returns\n",
    "\n",
    "# Initialize dictionaries to hold total number of links and sum of F-statistics\n",
    "eastward_links = {}\n",
    "westward_links = {}\n",
    "\n",
    "# Loop over each year for analysis\n",
    "for year in range(merged_data_filtered['DATETIME'].dt.year.min(), merged_data_filtered['DATETIME'].dt.year.max() + 1):\n",
    "    data_year = merged_data_filtered[(merged_data_filtered['DATETIME'].dt.year == year) & \n",
    "                                     (merged_data_filtered['DATETIME'].dt.month.isin([6, 7, 8]))]\n",
    "\n",
    "    # Replace zero values with a small quantity\n",
    "    small_quantity = 0.000001\n",
    "    data_year.loc[:, 'VALUE'] = data_year['VALUE'].replace(0, small_quantity)\n",
    "\n",
    "    # Calculate log returns\n",
    "    data_year['LOG_RETURN'] = data_year.groupby('STATION')['VALUE'].transform(calculate_log_returns)\n",
    "    data_year.dropna(inplace=True)\n",
    "\n",
    "    # Reset index to align with the original DataFrame\n",
    "    data_year.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Pivot the filtered data\n",
    "    pivot_data = data_year.pivot(index='DATETIME', columns='STATION', values='LOG_RETURN').fillna(0)\n",
    "\n",
    "    links = {}\n",
    "\n",
    "    # Analysis for each station pair\n",
    "    for station1 in pivot_data.columns:\n",
    "        for station2 in pivot_data.columns:\n",
    "            if station1 != station2:\n",
    "                f_statistic_out = granger_test(pivot_data, station1, station2)\n",
    "                if f_statistic_out > 0:\n",
    "                    links[(station1, station2)] = f_statistic_out\n",
    "\n",
    "                    # Determine if the link is eastward or westward\n",
    "                    coord1 = coordinates_data[coordinates_data['STATION'] == station1][['EST', 'NORD']].values[0]\n",
    "                    coord2 = coordinates_data[coordinates_data['STATION'] == station2][['EST', 'NORD']].values[0]\n",
    "                    if coord1[0] < coord2[0]:\n",
    "                        if year not in eastward_links:\n",
    "                            eastward_links[year] = 0\n",
    "                        eastward_links[year] += 1\n",
    "                    else:\n",
    "                        if year not in westward_links:\n",
    "                            westward_links[year] = 0\n",
    "                        westward_links[year] += 1\n",
    "\n",
    "# Prepare data for plotting\n",
    "years = sorted(list(set(westward_links.keys()).union(set(eastward_links.keys()))))\n",
    "number_of_westward_links = [westward_links.get(year, 0) for year in years]\n",
    "number_of_eastward_links = [eastward_links.get(year, 0) for year in years]\n",
    "\n",
    "# Plot the number of westward and eastward links over the years\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot westward links with a solid line\n",
    "plt.plot(years, number_of_westward_links, marker='o', linestyle='-', color='r', label='Westward Links')\n",
    "\n",
    "# Plot eastward links with a dashed line\n",
    "plt.plot(years, number_of_eastward_links, marker='o', linestyle='--', color='b', label='Eastward Links')\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('Autumn no of Granger Causality Links over Years (East vs West) - 24h', fontsize=16)\n",
    "plt.xlabel('Year', fontsize=14)\n",
    "plt.ylabel('Number of Links', fontsize=14)\n",
    "\n",
    "# Add grid and legend\n",
    "plt.grid(True)\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "# Show the plot\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "#plt.savefig('4.24Heast_west_links_over_years.jpg', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1a2cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#paiwise string visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "\n",
    "# Load data\n",
    "gauges_data = pd.read_csv('75gauges.csv')\n",
    "coordinates_data = pd.read_csv('merged_COO.csv')\n",
    "\n",
    "# Merge data on STATION\n",
    "merged_data = pd.merge(gauges_data, coordinates_data, on='STATION')\n",
    "\n",
    "# Convert 'DATETIME' to datetime\n",
    "merged_data['DATETIME'] = pd.to_datetime(merged_data['DATETIME'])\n",
    "\n",
    "# Define the station IDs you want to include in the network\n",
    "stations_of_interest = [779, 750, 684, 695, 718, 756, 706, 764, 729]\n",
    "\n",
    "# Filter merged_data to include only stations of interest\n",
    "merged_data_filtered = merged_data[merged_data['STATION'].isin(stations_of_interest)]\n",
    "\n",
    "# Granger causality test function\n",
    "def granger_test(dataframe, column1, column2, max_lag=1):\n",
    "    try:\n",
    "        gc_test = grangercausalitytests(dataframe[[column1, column2]], maxlag=max_lag)\n",
    "        f_statistic = gc_test[1][0]['ssr_ftest'][0]  # F-statistic\n",
    "        p_value = gc_test[1][0]['ssr_ftest'][1]  # p-value\n",
    "        return f_statistic if p_value < 0.01 else 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error in granger_test: {e}\")\n",
    "        return 0\n",
    "\n",
    "# Function to calculate log returns\n",
    "def calculate_log_returns(group):\n",
    "    log_returns = np.log(group) - np.log(group.shift(1))\n",
    "    return log_returns\n",
    "\n",
    "# Function to create the plot with nodes and directional arrows\n",
    "def create_network_plot(year, links, coordinates, title, filename):\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # Plot the nodes\n",
    "    for _, row in coordinates.iterrows():\n",
    "        if row['STATION'] in stations_of_interest:\n",
    "            ax.plot(row['EST'], row['NORD'], 'bo', markersize=2)  # blue circle for each node\n",
    "            ax.text(row['EST'], row['NORD'], row['LOCATION'], fontsize=12, ha='right')\n",
    "\n",
    "    # Plot the directional links\n",
    "    for (station1, station2), f_stat in links.items():\n",
    "        coord1 = coordinates[coordinates['STATION'] == station1][['EST', 'NORD']].values[0]\n",
    "        coord2 = coordinates[coordinates['STATION'] == station2][['EST', 'NORD']].values[0]\n",
    "        ax.annotate(\"\",\n",
    "                    xy=(coord2[0], coord2[1]), xycoords='data',\n",
    "                    xytext=(coord1[0], coord1[1]), textcoords='data',\n",
    "                    arrowprops=dict(arrowstyle=\"->\", color='blue', lw=1))\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.axis('off')\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    "# Initialize dictionary to hold F-statistics for the PA and CT stations\n",
    "f_stats_pa_ct = {}\n",
    "f_stats_ct_pa = {}\n",
    "\n",
    "# Loop over each year for analysis\n",
    "for year in range(merged_data_filtered['DATETIME'].dt.year.min(), merged_data_filtered['DATETIME'].dt.year.max() + 1):\n",
    "    data_year = merged_data_filtered[(merged_data_filtered['DATETIME'].dt.year == year) & \n",
    "                                     (merged_data_filtered['DATETIME'].dt.month.isin([12, 1, 2]))]\n",
    "\n",
    "    # Replace zero values with a small quantity\n",
    "    small_quantity = 0.000001\n",
    "    data_year['VALUE'] = data_year['VALUE'].replace(0, small_quantity)\n",
    "\n",
    "    # Calculate log returns\n",
    "    data_year['LOG_RETURN'] = data_year.groupby('STATION')['VALUE'].transform(calculate_log_returns)\n",
    "    data_year.dropna(inplace=True)\n",
    "\n",
    "    # Reset index to align with the original DataFrame\n",
    "    data_year.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Pivot the filtered data\n",
    "    pivot_data = data_year.pivot(index='DATETIME', columns='STATION', values='LOG_RETURN').fillna(0)\n",
    "\n",
    "    links = {}\n",
    "\n",
    "    # Analysis for each station pair\n",
    "    for station1 in pivot_data.columns:\n",
    "        for station2 in pivot_data.columns:\n",
    "            if station1 != station2:\n",
    "                f_statistic_out = granger_test(pivot_data, station1, station2)\n",
    "                if f_statistic_out > 0:\n",
    "                    links[(station1, station2)] = f_statistic_out\n",
    "\n",
    "                # Collect F-statistics for PA and CT stations\n",
    "                if station1 == 750 and station2 == 779:\n",
    "                    if year not in f_stats_pa_ct:\n",
    "                        f_stats_pa_ct[year] = f_statistic_out\n",
    "                if station1 == 779 and station2 == 750:\n",
    "                    if year not in f_stats_ct_pa:\n",
    "                        f_stats_ct_pa[year] = f_statistic_out\n",
    "\n",
    "    # Create plot for the year\n",
    "    title = f'Granger Causality Network for Summer {year}'\n",
    "    filename = f'0.gcnetwork_{year}.jpg'\n",
    "    create_network_plot(year, links, coordinates_data, title, filename)\n",
    "\n",
    "# Prepare data for heatmap\n",
    "years = sorted(list(f_stats_pa_ct.keys()))\n",
    "heatmap_data = pd.DataFrame({\n",
    "    'PA-TP': [f_stats_pa_ct[year] for year in years],\n",
    "    'TP-PA': [f_stats_ct_pa[year] for year in years]\n",
    "}, index=years)\n",
    "\n",
    "# Plot the heatmap with squared cells and explicit normalization\n",
    "plt.figure(figsize=(12, 2))  # Adjust the figure size to make it look like strips\n",
    "max_val = max(heatmap_data.values.flatten())\n",
    "sns.heatmap(heatmap_data.T, annot=False, cmap='coolwarm', cbar_kws={'label': 'F-statistic'}, vmin=0, vmax=max_val, square=True)\n",
    "plt.title('Winter Granger Causality F-statistics')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Direction')\n",
    "plt.yticks(rotation=0)\n",
    "plt.xticks(rotation=45) \n",
    "plt.savefig('1.PATP.jpg', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4cf069",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LINK 2D GRID seasonal\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "\n",
    "# Haversine formula to calculate the distance between two points on the Earth\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "    c = 2 * asin(sqrt(a))\n",
    "    r = 6371  # Radius of Earth in kilometers\n",
    "    return c * r\n",
    "\n",
    "# Function to perform Granger causality test and return F-statistic\n",
    "def granger_test(dataframe, column1, column2, max_lag=1):\n",
    "    try:\n",
    "        gc_test = grangercausalitytests(dataframe[[column1, column2]], maxlag=max_lag)\n",
    "        f_statistic = gc_test[1][0]['ssr_ftest'][0]  # F-statistic\n",
    "        p_value = gc_test[1][0]['ssr_ftest'][1]  # p-value\n",
    "        return f_statistic if p_value < 0.0001 else 0\n",
    "    except:\n",
    "        return 0  # Return 0 in case of an error\n",
    "\n",
    "# Function to determine the color of a cell based on the link direction and distance\n",
    "def link_color(lon1, lat1, lon2, lat2, max_distance):\n",
    "    distance = haversine(lon1, lat1, lon2, lat2)\n",
    "    normalized_distance = distance / max_distance  # Normalize distance to [0, 1]\n",
    "    \n",
    "    if lon2 > lon1:\n",
    "        # Eastward link, use red colormap\n",
    "        return mcolors.to_hex(plt.cm.Reds(normalized_distance))\n",
    "    else:\n",
    "        # Westward link, use blue colormap\n",
    "        return mcolors.to_hex(plt.cm.Blues(normalized_distance))\n",
    "\n",
    "# Load data\n",
    "gauges_data = pd.read_csv('75gauges.csv')\n",
    "coordinates_data = pd.read_csv('merged_COO.csv')\n",
    "\n",
    "# Merge data on STATION\n",
    "merged_data = pd.merge(gauges_data, coordinates_data, on='STATION')\n",
    "\n",
    "# Convert 'DATETIME' to datetime\n",
    "merged_data['DATETIME'] = pd.to_datetime(merged_data['DATETIME'])\n",
    "\n",
    "# Define the station order dictionary\n",
    "station_order = {\n",
    "    779: \"TP\",\n",
    "    775: \"ERICE\",\n",
    "    780: \"TRAPANI FULGATORE\",\n",
    "    778: \"SALEMI\",\n",
    "    774: \"CASTELVETRANO\",\n",
    "    773: \"CASTELLAMMARE DEL GOLFO\",\n",
    "    744: \"Contessa Entellina\",\n",
    "    751: \"PARTINICO\",\n",
    "    742: \"CAMPOREALE\",\n",
    "    749: \"MONREALE VIGNA API\",\n",
    "    745: \"CORLEONE\",\n",
    "    693: \"RIBERA\",\n",
    "    750: \"PA\",\n",
    "    686: \"BIVONA\",\n",
    "    748: \"MISILMERI\",\n",
    "    747: \"MEZZOJUSO\",\n",
    "    683: \"AG\",\n",
    "    755: \"TERMINI IMERESE\",\n",
    "    685: \"ARAGONA\",\n",
    "    684: \"AGRIGENTO MANDRASCAVA\",\n",
    "    687: \"CAMMARATA\",\n",
    "    740: \"ALIA\",\n",
    "    689: \"CANICATTì\",\n",
    "    700: \"MUSSOMELI\",\n",
    "    703: \"SCLAFANI BAGNI\",\n",
    "    690: \"LICATA\",\n",
    "    746: \"LASCARI\",\n",
    "    696: \"DELIA\",\n",
    "    754: \"POLIZZI GENEROSA\",\n",
    "    753: \"PETRALIA SOTTANA\",\n",
    "    695: \"CL\",\n",
    "    701: \"RIESI\",\n",
    "    743: \"CASTELBUONO\",\n",
    "    718: \"EN\",\n",
    "    752: \"GANGI\",\n",
    "    699: \"MAZZARINO\",\n",
    "    736: \"PETTINEO\",\n",
    "    731: \"MISTRETTA\",\n",
    "    722: \"PIAZZA ARMERINA\",\n",
    "    762: \"ACATE\",\n",
    "    721: \"NICOSIA\",\n",
    "    717: \"AIDONE\",\n",
    "    723: \"CARONIA BUZZA\",\n",
    "    760: \"SANTA CROCE CAMERINA\",\n",
    "    710: \"MAZZARRONE\",\n",
    "    716: \"CALTAGIRONE\",\n",
    "    757: \"COMISO\",\n",
    "    737: \"SAN FRATELLO\",\n",
    "    730: \"MILITELLO ROSMARINO\",\n",
    "    761: \"SCICLI\",\n",
    "    756: \"RG\",\n",
    "    725: \"CESARò VIGNAZZA\",\n",
    "    711: \"MINEO\",\n",
    "    733: \"NASO\",\n",
    "    705: \"BRONTE\",\n",
    "    712: \"PATERNò\",\n",
    "    770: \"PALAZZOLO ACREIDE\",\n",
    "    709: \"MALETTO\",\n",
    "    765: \"FRANCOFONTE\",\n",
    "    766: \"LENTINI\",\n",
    "    715: \"RANDAZZO\",\n",
    "    758: \"ISPICA\",\n",
    "    735: \"PATTI\",\n",
    "    713: \"PEDARA\",\n",
    "    767: \"NOTO\",\n",
    "    706: \"CT\",\n",
    "    769: \"PACHINO\",\n",
    "    708: \"LINGUAGLOSSA\",\n",
    "    734: \"NOVARA DI SICILIA\",\n",
    "    764: \"SR\",\n",
    "    707: \"RIPOSTO\",\n",
    "    739: \"TORREGROTTA\",\n",
    "    738: \"SAN PIER NICETO\",\n",
    "    727: \"FIUMEDINISI\",\n",
    "    729: \"ME\"\n",
    "}\n",
    "\n",
    "\n",
    "# Create a list of station IDs in order\n",
    "station_list = list(station_order.keys())\n",
    "\n",
    "# Process data for each year and calculate all F-statistics\n",
    "all_f_statistics = []\n",
    "for year in range(merged_data['DATETIME'].dt.year.min(), merged_data['DATETIME'].dt.year.max() + 1):\n",
    "    data_year = merged_data[(merged_data['DATETIME'].dt.year == year) & (merged_data['DATETIME'].dt.month.isin([9, 10, 11]))]\n",
    "    pivot_data = data_year.pivot(index='DATETIME', columns='STATION', values='VALUE').fillna(0)\n",
    "\n",
    "    for station1 in pivot_data.columns:\n",
    "        for station2 in pivot_data.columns:\n",
    "            if station1 != station2:\n",
    "                f_statistic = granger_test(pivot_data, station1, station2)\n",
    "                all_f_statistics.append(f_statistic)\n",
    "\n",
    "# Calculate the 99th percentile of F-statistics\n",
    "f_statistic_99th_percentile = np.percentile(all_f_statistics, 99)\n",
    "\n",
    "# Initialize a matrix to store the link information for each year\n",
    "years = range(merged_data['DATETIME'].dt.year.min(), merged_data['DATETIME'].dt.year.max() + 1)\n",
    "num_stations = len(station_list)\n",
    "link_matrix = {year: np.zeros((num_stations, num_stations)) for year in years}\n",
    "\n",
    "# Populate the link matrix\n",
    "for year in years:\n",
    "    data_year = merged_data[(merged_data['DATETIME'].dt.year == year) & (merged_data['DATETIME'].dt.month.isin([9, 10,11]))]\n",
    "    pivot_data = data_year.pivot(index='DATETIME', columns='STATION', values='VALUE').fillna(0)\n",
    "\n",
    "    for i, station1 in enumerate(station_list):\n",
    "        for j, station2 in enumerate(station_list):\n",
    "            if station1 != station2:\n",
    "                f_statistic = granger_test(pivot_data, station1, station2)\n",
    "                if f_statistic > f_statistic_99th_percentile:\n",
    "                    link_matrix[year][i, j] = f_statistic\n",
    "\n",
    "# Create a dictionary mapping station IDs to their coordinates\n",
    "station_coordinates = coordinates_data.set_index('STATION')[['EST', 'NORD']].to_dict('index')\n",
    "\n",
    "# Get maximum distance\n",
    "max_distance = 0\n",
    "for i in range(len(station_list)):\n",
    "    for j in range(i+1, len(station_list)):\n",
    "        lon1, lat1 = station_coordinates[station_list[i]]['EST'], station_coordinates[station_list[i]]['NORD']\n",
    "        lon2, lat2 = station_coordinates[station_list[j]]['EST'], station_coordinates[station_list[j]]['NORD']\n",
    "        dist = haversine(lon1, lat1, lon2, lat2)\n",
    "        if dist > max_distance:\n",
    "            max_distance = dist\n",
    "\n",
    "# Create and display the grids\n",
    "for year in years:\n",
    "    grid = np.ones((num_stations, num_stations, 3))  # Initialize grid with white background (no links)\n",
    "\n",
    "    for i, station1 in enumerate(station_list):\n",
    "        lon1, lat1 = station_coordinates[station1]['EST'], station_coordinates[station1]['NORD']\n",
    "        for j, station2 in enumerate(station_list):\n",
    "            if link_matrix[year][i, j] > 0:  # Check if there's a significant link\n",
    "                lon2, lat2 = station_coordinates[station2]['EST'], station_coordinates[station2]['NORD']\n",
    "                color = link_color(lon1, lat1, lon2, lat2, max_distance)\n",
    "                grid[i, j] = mcolors.to_rgb(color)\n",
    "\n",
    "    # Create and display the plot\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(grid, interpolation='nearest')\n",
    "    plt.title(f'Autumn Adjacency Matrix for {year}')\n",
    "    plt.xticks(range(num_stations), [station_order[s] for s in station_list], rotation=90)\n",
    "    plt.yticks(range(num_stations), [station_order[s] for s in station_list])\n",
    "    plt.grid(False)\n",
    "    plt.savefig(f\"4.Grid_{year}autumn.jpg\", bbox_inches='tight', dpi=300)\n",
    "\n",
    "    plt.show()\n",
    "    #GRANGER2DGRID4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d53f12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Iterate over years\n",
    "for year in years:\n",
    "    # Initialize figure and axis\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # Set up x, y, z coordinates for bars and their colors\n",
    "    x_coords = []\n",
    "    y_coords = []\n",
    "    z_coords = []\n",
    "    f_values = []\n",
    "    colors = []\n",
    "\n",
    "    # Populate coordinates, F-values, and colors\n",
    "    for i, station1 in enumerate(station_list):\n",
    "        lon1, lat1 = station_coordinates[station1]['EST'], station_coordinates[station1]['NORD']\n",
    "        for j, station2 in enumerate(station_list):\n",
    "            if link_matrix[year][i, j] > 0:  # Check if there's a significant link\n",
    "                lon2, lat2 = station_coordinates[station2]['EST'], station_coordinates[station2]['NORD']\n",
    "                x_coords.append(i)\n",
    "                y_coords.append(j)\n",
    "                z_coords.append(0)  # Z-coordinate for the base of the bar (start at 0)\n",
    "                f_values.append(link_matrix[year][i, j])  # F-statistic value\n",
    "                # Determine color based on link direction and distance\n",
    "                color = link_color(lon1, lat1, lon2, lat2, max_distance)\n",
    "                colors.append(color)\n",
    "\n",
    "    # Plot bars with colors\n",
    "    for x, y, z, f, color in zip(x_coords, y_coords, z_coords, f_values, colors):\n",
    "        ax.bar3d(x, y, z, 1, 1, f, color=color, zsort='average')\n",
    "\n",
    "    # Set labels and title\n",
    "    ax.set_xticks(range(num_stations))\n",
    "    ax.set_xticklabels([station_order[s] for s in station_list], rotation=90)\n",
    "    ax.set_yticks(range(num_stations))\n",
    "    ax.set_yticklabels([station_order[s] for s in station_list])\n",
    "    ax.set_xlabel('Station')\n",
    "    ax.set_ylabel('Station')\n",
    "    ax.set_zlabel('F-Statistic')\n",
    "    ax.set_title(f'Autumn Adjacency Matrix for {year}')\n",
    "\n",
    "    plt.savefig(f\"4.3D_grid_{year}autumn.jpg\", bbox_inches='tight', dpi=300)\n",
    "\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
